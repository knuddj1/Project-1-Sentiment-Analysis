{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Report -  Dean Knudson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my third year project I have continued working with David Rozado on a Sentiment Analysis Engine.\n",
    "\n",
    "Last semester, as part of a special topic study, I began learning about, and creating models for, Sentiment Analysis. The intentions of this project were to create a model that would correctly predict the sentiment of a given piece of text 80% of time. David offered for me to continue working on the problem this semester. Due to to the fact I didn't achieve the result above and the availability of recent advancements in the field of NLP, I accepted.\n",
    "\n",
    "More details on my work last semester can be found \n",
    "[here](https://github.com/knuddj1/Sentiment-Analysis/blob/master/recap%20stuff/semester_recap.ipynb)\n",
    "\n",
    "The official objective of this project is to create a Sentiment Analysis Engine that is able to correctly classify the sentiment of a piece of text into 1 of 3 possible classes (negative, neutral, positive) at a greater, equal or close to equal to accuracy of 80% that david set for me last semester. \n",
    "\n",
    "This notebook covers the processes I underwent as part of this project. Each section is listed below:\n",
    "\n",
    "1. [Dataset](#Dataset)\n",
    "    - [Data Collection](#Data-Collection)\n",
    "    - [Constructing a Custom Dataset](#Constructing-a-Custom-Dataset)\n",
    "    - [GUI Application](#GUI-Application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [Modeling](#Modeling)\n",
    "    - [Establishing Baselines](#Establishing-Baselines)\n",
    "    - [BERT](#BERT)\n",
    "    - [bert-as-service](#bert-as-service)\n",
    "    - [Cosine-Similarity](#Cosine-Similarity)\n",
    "    - [Grid Search](#Grid-Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [Development](#Development)\n",
    "    - [Technical/Professional Proficiency](#Technical/Professional-Proficiency)\n",
    "    - [Voluntary](#Voluntary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intial task of a machine learning project is to obtain a dataset, or in this case I used **mutliple** datasets and created a new dataset out of these combined.\n",
    "\n",
    "David advised me to read [this](https://monkeylearn.com/sentiment-analysis/) website as a refresher to refamiliarize myself with the world of sentiment analysis. This website also offered download links to their favourite datasets for experimenting with sentiment analysis and a machine learning approach.\n",
    "\n",
    "#### Datasets found at monkeylearn.com:\n",
    "    \n",
    "[Amazon Food Reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews/)\n",
    ">*\"This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all 500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\"*\n",
    "\n",
    "[Amazon Reviews for Sentiment Analysis](https://www.kaggle.com/bittlingmayer/amazonreviews/)\n",
    ">*\"This dataset consists of a few million Amazon customer reviews (input text) and star ratings (output labels) for learning how to train fastText for sentiment analysis.\n",
    "The idea here is a dataset is more than a toy - real business data on a reasonable scale - but can be trained in minutes on a modest laptop.\"*\n",
    "\n",
    "[Twitter US Airline Sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment/)\n",
    ">*\"A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets found elsewhere:\n",
    "\n",
    "[sentiment140](http://help.sentiment140.com/)\n",
    ">*\"It contains 1,600,000 tweets extracted using the twitter api. The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment. Sentiment140 was created by Alec Go, Richa Bhayani, and Lei Huang, who were Computer Science graduate students at Stanford University.\"*\n",
    "\n",
    "[Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    ">*\"Obtained from IMDB movie reviews, this is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considered but not used\n",
    "\n",
    "Both of these were found on monkeylearn.com.\n",
    "\n",
    "[First GOP Debate Twitter Sentiment](https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment/)\n",
    ">*\"We looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both sentiment analysis and data categorization. Contributors were asked if the tweet was relevant, which candidate was mentioned, what subject was mentioned, and then what the sentiment was for a given tweet. We've removed the non-relevant messages from the uploaded dataset\"*\n",
    "\n",
    "I chose not to use this dataset because lots of the samples in this dataset were not correctly labeled. It was unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yelp Dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset)\n",
    ">*\"This dataset is a subset of Yelp's businesses, reviews, and user data. It was originally put together for the Yelp Dataset Challenge which is a chance for students to conduct research or analysis on Yelp's data and share their discoveries. In the dataset you'll find information about businesses across 11 metropolitan areas in four countries.\"*\n",
    "\n",
    "I didnt use this dataset because I couldnt get it to load properly. There was something wrong with the file that prevented me from actually parsing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Constructing a Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task, after sourcing the datasets, was to combine them all into a single dataset. I originally planned for this part of the project to take only about a week. However, it ended up taking about four weeks as I ended up building a GUI application that I will be able to use in the future with other projects involving multiple datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration File\n",
    "\n",
    "\n",
    "I created a configuration file to hold all the parameters for the creation of a dataset, as well as the configs for loading each individual dataset. Below is the format of an example configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    {\n",
    "        \"DATASET_NAME\": \"Example Dataset Name\",\n",
    "        \"DATASET_FILE_EXTENSION\": \"csv\",\n",
    "        \"TEST_SET_SIZE\": 0.2,\n",
    "        \"SHUFFLE\": true,\n",
    "        \"NUM_SHUFFLES\": 10,\n",
    "        \"CONCAT_TYPE\": \"equal\",\n",
    "        \"CONFIGS\": {\n",
    "            \"Example Dataset 1\": {\n",
    "                \"DATASET_PATH\": \"dataset/path/example1.zip\",\n",
    "                \"LOADING_SCRIPT\": \"loading/script/path/example1.py\",\n",
    "                \"PERCENT\": 0.6,\n",
    "                \"OTHER_PARAMS\": null\n",
    "            },\n",
    "            \"Example Dataset 2\": {\n",
    "                \"DATASET_PATH\": \"dataset/path/example2.zip\",\n",
    "                \"LOADING_SCRIPT\": \"loading/script/path/example2.py\",\n",
    "                \"PERCENT\": 0.9,\n",
    "                \"OTHER_PARAMS\": {\n",
    "                    \"example_param1\": 100,\n",
    "                    \"example_param2\": \"example text\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Scripts\n",
    "\n",
    "Each dataset came in its own format, inside its own specific filetype. Some datasets had redundant data. To solve this issue I created a loading script for each dataset. The loading scripts consisted of a single function called *load* which contains instructions to extract a specific dataset and the returns that dataset as list of dictionaries in the format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [\n",
    "        {\n",
    "            input: example text 1\n",
    "            label: example classification label 1\n",
    "        },\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        {\n",
    "            input: example text N\n",
    "            label: example classification label N\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "After each dataset is loaded and in the same format displayed above, they are then concatenated into a single dataset. I created two different variations of concatenating the datasets listed below:\n",
    "\n",
    " - **Equal Concatenation**\n",
    " \n",
    "    First the the number of samples in the dataset with the least samples is found. Then that number of samples is retrieved from each of the other datasets. Each of these new subsets are then split by a parameter _test-split_, a decimal number between 0 - 0.5. The percentage of the subset split by this parameter is saved into a csv-file with name \"DATASET_test.csv\". The remainder of the subset is placed into the training set.\n",
    "\n",
    "- **Percentage Concatenation**\n",
    "\n",
    "    This is the same as equal concatenation above except that instead of using the number of samples in the smallest dataset, the percentage parameter from each datasets config is used instead. Therefore if example dataset A's config has a percentage of 0.7, 70% of that dataset will be used.\n",
    "\n",
    "Each dataset will be shuffled _n_ times before subsets are extracted, if the shuffle parameter in the configuration file is set to true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### GUI Application\n",
    "\n",
    "In an effort to organise my thoughts and to make creating various custom datasets an easier, I went on somewhat of a tangent and created a GUI interface for this process. I used a python package Tkinter, which is part of the standard library to create the GUI. The section that follows is a brief walk through of the process of creating a custom dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The intial view prompts the user to either create a new configuration file or load an existing one. Loading an existing configuration file follows the same process as seen below however all settings and data-configs are loaded. I added this feature to allow me to quickly change settings and add/change/delete datasets to the custom dataset without having to re-enter all the information each time, as that was rather time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/view1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After pressing the New or Load buttons the user is presented with this view. This is where the settings that affect how the custom dataset shapes up. The behaviours of each field are:\n",
    " \n",
    " - Dataset name: What the final custom dataset will be named\n",
    " - Dataset File Extension: Either csv or json, this is the file type that the custom dataset will be saved under\n",
    " - Test set size: This is how much of the custom dataset will be used for testing\n",
    " - Shuffle Dataset: Whether to shuffle each individual dataset before they are split and concatenated\n",
    " - Number shuffles: How many times to shuffle each individual dataset\n",
    " - Dataset Concatenation Type: The method to split and concatenate individual datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/view2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This view is presented when the user chooses to proceed from the settings view. Note that the user can choose to go back and edit the settings again at any time by pressing the settings button in the top left. The user then must create at least one new dataset config, by pressing the big add new data config button, to proceed any further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/view3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pressing the add new data config button on the previous slide presents the user with this view. This is where individual dataset information is added. The name of the dataset, a path to the dataset and a path to the loading script for that dataset must be supplied. The other parameters are optional and have preset defaults. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/view4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. When a data config has been submitted it is added to a list of data configs back on the previous view. The user can add another data config by pressing the add new data config button again. Users can edit existing configs by pressing the name of the config or delete them by pressing the delete button. Once the user is ready to proceed pressing the create new dataset will take them to the next view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/view5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. This is the final view ,it gives feedback on which dataset is being loaded, when the dataset is being split and concatenated and when its finished. Once this view is finished the program will close and the user can find their custom dataset in the directory they started the program from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/view6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This application still has alot of stuff to work through but it does what I need it too right now so its good enough. Its also not a complete waste of time as I will use this to kick start future projects where im dealing with multiple datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solid baseline on any machine learning project is a great reference when attemping to validate a future models performance. If I was to use a random baseline for this project I would expect to get results of around 33% because there are 3 distinct categories.\n",
    "\n",
    "To validate future models I chose to use four baselines. Three of these were python recommended by David which he was using in his research and the other being the best model from my project last semester. Below are the results of these models tested the five test-sets that I generated during the dataset construction phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAFgCAYAAABNIolGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xt4VdWd//H3l4BSRdR6ZUQFW4oKyC3gjYtWKdhaFJVyUStjK7UWrTcqjtaitn2stmPHqVaxdbCKP1CGWuw4XigiMlIh2KCAIhexRFFRCgqIclm/P3JIDzGQKNnkBN6v58nD2Xuvvfb3nJyQ5JO11o6UEpIkSZIkSYWsQV0XIEmSJEmSVB0DDEmSJEmSVPAMMCRJkiRJUsEzwJAkSZIkSQXPAEOSJEmSJBU8AwxJkiRJklTwDDAkSdJnFhFDImJa3vbqiDiiLmuSJEk7NwMMSZLquYhYEhEf5UKEf0TE/0TEoTuyhpRSk5TS4qz6j4jREbEhIv4lq2tIkqTCZoAhSdLO4ZsppSZAM+Ad4D/ruJ5aExF7AmcDq4Bzd/C1G+7I60mSpK0zwJAkaSeSUloHjAeO3rwvIr4REX+LiA8iYmlEjMw71jgiHoyI9yNiZUTMjIiDcsf2jojfR8SyiHgzIn4aEUVVXTciUkR8Ofd4dETcmRsJ8mFEvBARX8pre2REPB0RKyJifkR8q5qndTawErgJuKDSdYsi4t8iYlHuWrM2jz6JiDZ513knIv4tr76f5vVxUkSU5W0viYhrIuIlYE1ENIyIEXnXmBcR/SrVcVFEvJJ3vFNEDI+I/67U7j8j4tfVPF9JklQFAwxJknYiEbEHMAD4a97uNcC3gX2AbwDfj4gzc8cuAPYGDgX2Ay4GPsodux/YAHwZ6Ah8DfhuDUsZBNwI7AssBH6Wq29P4GngIeDAXLu7IqLNNvq6APh/wFjgyIjolHfsylwfXweaAhcCayNiL2AS8ATwL7nn8Jca1r65/m8A+6SUNgCLgO6Uv1Y3Ag9GRLPcc+oPjKT8NW4K9AXeBx4E+kTEPrl2DSn/3DzwGeqQJEk5BhiSJO0cHo2IlcAHQC/gts0HUkpTUkovp5Q2pZReojwM6Jk7vJ7y4OLLKaWNKaVZKaUPcqMwTgMuTymtSSm9C9wODKxhPRNSSjNyv/yPATrk9p8OLEkp/VdKaUNK6UXgv4FzquokIg4DTgYeSim9Q3kIkT8K47vA9Sml+anc7JTS+7nrvJ1S+lVKaV1K6cOU0gs1rB3gjpTS0pTSRwAppUdSSm/lXsNxwAKga14Nt6aUZuZqWJhSeiOltAyYCvTPtesDvJdSmvUZ6pAkSTkGGJIk7RzOTCntA+wODAOejYiDASLi2Ih4JiKWR8QqykdZ7J877wHgSWBsRLwVEbdGRCPgcKARsCw3tWQlcA/loyZq4u28x2uBJrnHhwPHbu4z1++5wMFb6ed84JWUUmluewwwOFcjlI8cWVTFeVvbX1NL8zci4tsRUZpXc1v++Rpu61r3A+flHp+Hoy8kSfrcDDAkSdqJ5EZRTAA2At1yux8CJgKHppT2Bu4GItd+fUrpxpTS0cAJlI9c+Dblv8B/DOyfUton99E0pbStqR41sRR4Nq/PfXJ3MPn+Vtp/GzgiIt6OiLeBf6c8ODgtr78vVXHe1vZD+ZSaPfK2qwpP0uYHEXE4cC/lwdB+uaBoDrnXsJprPQocExFtKX9tx2ylnSRJqoYBhiRJO5Eodwbla0+8ktu9F7AipbQuIroCg/PanxwR7XKLc35A+ZSSjbnpD08Bv4qIphHRICK+FBE92T5/Br4SEedHRKPcR5eIOKqK53I85cFAV8qnoHSgfOTDQ/xzGsnvgJsjolXuuR8TEfvlrnNwRFweEbtHxF4RcWzunFLg6xHxxdwolcurqXlPygON5bm6/jVXx2a/A66OiM65Gr6cCz3yF1V9CJiRUvr7Z3mxJEnSPxlgSJK0c3gsIlZTHkL8DLggpTQ3d+wS4KaI+BC4AXg477yDKf8F+wPKA49nKV98EspHP+wGzAP+kWvXbHuKTCl9SPlioAOBtyifavILyqe+VHYB8Kfc+h1vb/4A/gM4PSK+SPmIjIcpD1s+AH4PfCF3nV7AN3PXWED5WhpQPo1jNrAkd964amqeB/wKmE75LWrbAf+Xd/wRyl/zh4APKR918cW8Lu7PneP0EUmStkOklKpvJUmSpM8ltxDpq8DBKaUP6roeSZLqK0dgSJIkZSQiGlB+q9exhheSJG2fhnVdgCRJ0s4oIvakfMrJG5TfQlWSJG0Hp5BIkiRJkqSC5xQSSZIkSZJU8OrdFJL9998/tWjRoq7LkCRJkiRJtWDWrFnvpZQOqK5dvQswWrRoQUlJSV2XIUmSJEmSakFEvFGTdk4hkSRJkiRJBc8AQ5IkSZIkFTwDDEmSJEmSVPDq3RoYkiRJkqTCsn79esrKyli3bl1dl6IC1rhxY5o3b06jRo0+1/mZBhgR0Qf4D6AI+F1K6ZZKxw8D7gf2ybUZkVJ6PMuaJEmSJEm1q6ysjL322osWLVoQEXVdjgpQSon333+fsrIyWrZs+bn6yGwKSUQUAXcCpwFHA4Mi4uhKza4HHk4pdQQGAndlVY8kSZIkKRvr1q1jv/32M7zQVkUE++2333aN0slyDYyuwMKU0uKU0ifAWOCMSm0S0DT3eG/grQzrkSRJkiRlxPBC1dne90iWAcYhwNK87bLcvnwjgfMiogx4HLi0qo4iYmhElEREyfLly7OoVZIkSZIkFbAs18CoKlpJlbYHAaNTSr+KiOOBByKibUpp0xYnpTQKGAVQXFxcuQ9JkiRJUgEpLq7d/kpKarc/1U9ZjsAoAw7N227Op6eIfAd4GCClNB1oDOyfYU2SJEmSpJ3MypUrueuuz7ek4pIlS3jooYcqtkePHs2wYcOqbNukSZNt9jVlyhROP/30z1VHTUycOJFbbrml+oZVGDJkCOPHj6/linasLAOMmUCriGgZEbtRvkjnxEpt/g6cAhARR1EeYDhHRJIkSZJUY7UZYBSyvn37MmLEiLouo85kFmCklDYAw4AngVcov9vI3Ii4KSL65ppdBVwUEbOB/wcMSSk5RUSSJEmSVGMjRoxg0aJFdOjQgeHDh3PbbbfRpUsXjjnmGH7yk58AMHPmTI455hjWrVvHmjVraNOmDXPmzGHEiBE899xzdOjQgdtvvx2ApUuX0qdPH1q3bs2NN974qeullBg+fDht27alXbt2jBs3ruLYBx98QL9+/Tj66KO5+OKL2bRp06fO36xJkyZcd911tG/fnuOOO4533nkHgMcee4xjjz2Wjh07cuqpp1bs3zw6ZNWqVbRo0aKi77Vr13LooYeyfv16Fi1aRJ8+fejcuTPdu3fn1VdfrbjepEmT6N69O1/5ylf485//DJQHON27d6dTp0506tSJ559/HoDzzz+fP/3pTxXnnnvuuUycOJGNGzcyfPjwitf3nnvuAWDZsmX06NGDDh060LZtW5577rnP+FmsXpZrYJBSepzyxTnz992Q93gecGKWNUiSVAhqey5wbXJesSSpvrvllluYM2cOpaWlPPXUU4wfP54ZM2aQUqJv375MnTqVHj160LdvX66//no++ugjzjvvPNq2bcstt9zCL3/5y4pf6EePHs2MGTOYM2cOe+yxB126dOEb3/gGxXnfzCdMmEBpaSmzZ8/mvffeo0uXLvTo0QOAGTNmMG/ePA4//HD69OnDhAkTOOecc6qse82aNRx33HH87Gc/40c/+hH33nsv119/Pd26deOvf/0rEcHvfvc7br31Vn71q19VnLf33nvTvn17nn32WU4++WQee+wxevfuTaNGjRg6dCh33303rVq14oUXXuCSSy5h8uTJQHlY8eyzz7Jo0SJOPvlkFi5cyIEHHsjTTz9N48aNWbBgAYMGDaKkpITvfve73H777ZxxxhmsWrWK559/nvvvv5/f//737L333sycOZOPP/6YE088ka997WtMmDCB3r17c91117Fx40bWrl1b65/nTAMMSZIkSZJ2pKeeeoqnnnqKjh07ArB69WoWLFhAjx49uOGGG+jSpQuNGzfmjjvu2GofvXr1Yr/99gPgrLPOYtq0aVsEGNOmTWPQoEEUFRVx0EEH0bNnT2bOnEnTpk3p2rUrRxxxBACDBg1i2rRpWw0wdtttt4o1Mzp37szTTz8NQFlZGQMGDGDZsmV88skntGzZ8lPnDhgwgHHjxnHyySczduxYLrnkElavXs3zzz9P//79K9p9/PHHFY+/9a1v0aBBA1q1asURRxzBq6++SsuWLRk2bBilpaUUFRXx2muvAdCzZ09+8IMf8O677zJhwgTOPvtsGjZsyFNPPcVLL71UsZ7GqlWrWLBgAV26dOHCCy9k/fr1nHnmmXTo0KGaz9RnZ4AhSZIkSdpppJS49tpr+d73vvepYytWrGD16tWsX7+edevWseeee1bZR0Rsc3tbKx9Ud26+Ro0aVRwvKipiw4YNAFx66aVceeWV9O3blylTpjBy5MhPndu3b1+uvfZaVqxYwaxZs/jqV7/KmjVr2GeffSgtLa1xbbfffjsHHXQQs2fPZtOmTTRu3Lji+Pnnn8+YMWMYO3Ys9913X8Vz/8///E969+79qf6nTp3K//zP/3D++eczfPhwvv3tb2/1uX8eWS7iKUmSJEnaBZWU1O5Hdfbaay8+/PBDAHr37s19993H6tWrAXjzzTd59913ARg6dCg333wz5557Ltdcc82nzt3s6aefZsWKFXz00Uc8+uijnHjilisf9OjRg3HjxrFx40aWL1/O1KlT6dq1K1A+heT1119n06ZNjBs3jm7dun3m12/VqlUccsghANx///1VtmnSpAldu3blhz/8IaeffjpFRUU0bdqUli1b8sgjjwDlYcPs2bMrznnkkUfYtGkTixYtYvHixbRu3ZpVq1bRrFkzGjRowAMPPMDGjRsr2g8ZMoRf//rXALRp06bi9f3tb3/L+vXrAXjttddYs2YNb7zxBgceeCAXXXQR3/nOd3jxxRc/8/OujiMwJEmSJEn12n777ceJJ55I27ZtOe200xg8eDDHH388UP6L/oMPPsgTTzxBw4YNGTx4MBs3buSEE05g8uTJdO/enYYNG9K+fXuGDBnCvvvuS7du3Tj//PNZuHAhgwcP3mL6CEC/fv2YPn067du3JyK49dZbOfjgg3n11Vc5/vjjGTFiBC+//DI9evSgX79+n/n5jBw5kv79+3PIIYdw3HHH8frrr1fZbsCAAfTv358pU6ZU7BszZgzf//73+elPf8r69esZOHAg7du3B6B169b07NmTd955h7vvvpvGjRtzySWXcPbZZ/PII49w8sknbzEq5aCDDuKoo47izDPPrNj33e9+lyVLltCpUydSShxwwAE8+uijTJkyhdtuu41GjRrRpEkT/vCHP3zm512dqG83/SguLk4lrjYmSapnXMRTkrQze+WVVzjqqKPqugzVsrVr19KuXTtefPFF9t5771rps6r3SkTMSilV+9OSU0gkSZIkSdIWJk2axJFHHsmll15aa+HF9nIKiSRJkiRJGTr22GO3uBsIwAMPPEC7du3qqKLqnXrqqfz973+v6zK2YIAhSZIkSVKGXnjhhbouYafgFBJJkiRJklTwDDAkSZIkSVLBM8CQJEmSJEkFzzUwJEmSJEm1qnhU7d4/vGSo9/yWIzAkSZIkSfXcypUrueuuuz7XuUuWLOGhhx6q2B49ejTDhg2rsm2TJk222deUKVM4/fTTP1cdNfHoo48yb968iu3Ro0fz1ltvZXa9QmOAIUmSJEmq12ozwChktRFgbNiwobbL2mEMMCRJkiRJ9dqIESNYtGgRHTp0YPjw4dx222106dKFY445hp/85CcAzJw5k2OOOYZ169axZs0a2rRpw5w5cxgxYgTPPfccHTp04Pbbbwdg6dKl9OnTh9atW3PjjTd+6nopJYYPH07btm1p164d48aNqzj2wQcf0K9fP44++mguvvhiNm3atNW6mzRpwlVXXUWnTp045ZRTWL58OQD33nsvXbp0oX379px99tmsXbuW559/nokTJzJ8+HA6dOjAL37xC0pKSjj33HPp0KEDH330EbNmzaJnz5507tyZ3r17s2zZMgBOOukk/u3f/o2ePXvyH//xHwwZMoTLLruME044gSOOOILx48fX2uciSwYYkiRJkqR67ZZbbuFLX/oSpaWl9OrViwULFjBjxgxKS0uZNWsWU6dOpUuXLvTt25frr7+eH/3oR5x33nm0bduWW265he7du1NaWsoVV1wBwIwZMxgzZgylpaU88sgjlJRsuQbHhAkTKC0tZfbs2UyaNInhw4dXhAUzZszgV7/6FS+//DKLFi1iwoQJW617zZo1dOrUiRdffJGePXtWhCVnnXUWM2fOZPbs2Rx11FH8/ve/54QTTqBv377cdtttlJaWcs0111BcXFxRZ8OGDbn00ksZP348s2bN4sILL+S6666ruNbKlSt59tlnueqqqwBYtmwZ06ZN489//jMjRoyo1c9HVlzEU5IkSZK003jqqad46qmn6NixIwCrV69mwYIF9OjRgxtuuIEuXbrQuHFj7rjjjq320atXL/bbbz+gPEyYNm0axcX/XJh02rRpDBo0iKKiIg466CB69uzJzJkzadq0KV27duWII44AYNCgQUybNo1zzjmnyus0aNCAAQMGAHDeeedx1llnATBnzhyuv/56Vq5cyerVq+ndu3e1z3v+/PnMmTOHXr16AbBx40aaNWtWcXzzdTY788wzadCgAUcffTTvvPNOtf0XAgMMSZIkSdJOI6XEtddey/e+971PHVuxYgWrV69m/fr1rFu3jj333LPKPiJim9sppa1ev7pzt2Vz2yFDhvDoo4/Svn17Ro8ezZQpU6o9N6VEmzZtmD59epXHKz/X3XfffYtz6wMDDOlzKCku3Ns4FZfU7i2rJEmSpM9qR9/2dK+99uLDDz8EoHfv3vz4xz/m3HPPpUmTJrz55ps0atSIAw88kKFDh3LzzTfz+uuvc8011/Cb3/xmi3M3e/rpp1mxYgVf+MIXePTRR7nvvvu2ON6jRw/uueceLrjgAlasWMHUqVO57bbbePXVV5kxYwavv/46hx9+OOPGjWPo0KFbrXvTpk2MHz+egQMH8tBDD9GtWzcAPvzwQ5o1a8b69esZM2YMhxxyyKeeZ+Xt1q1bs3z5cqZPn87xxx/P+vXree2112jTps32v8AFwgBDkiRJklSv7bfffpx44om0bduW0047jcGDB3P88ccD5QtlPvjggzzxxBM0bNiQwYMHs3HjRk444QQmT55M9+7dadiwIe3bt2fIkCHsu+++dOvWjfPPP5+FCxcyePDgLaaPAPTr14/p06fTvn17IoJbb72Vgw8+mFdffZXjjz+eESNG8PLLL9OjRw/69eu31br33HNP5s6dS+fOndl7770rFgO9+eabOfbYYzn88MNp165dRUgxcOBALrroIu644w7Gjx/PkCFDuPjii/nCF77A9OnTGT9+PJdddhmrVq1iw4YNXH755TtVgBH1ZajIZsXFxanyAirSjuYIDEmfVXEBf2n6bVWStL1eeeUVjjrqqLouo95p0qQJq1evrusydqiq3isRMSulVO1PS96FRJIkSZIkFTynkEiSJEmSlKFjjz2Wjz/+eIt9DzzwwC43+mJ7GWBIkiRJkpShF154oa5L2Ck4hUSSJEmSJBU8AwxJkiRJklTwMg0wIqJPRMyPiIURMaKK47dHRGnu47WIWJllPZIkSZIkqX7KbA2MiCgC7gR6AWXAzIiYmFKat7lNSumKvPaXAh2zqkeSJEmStIPU9v3DM7zn98iRI2nSpAlXX311ZtdQ7chyBEZXYGFKaXFK6RNgLHDGNtoPAv5fhvVIkiRJkrRdNmzYUNcl7LKyDDAOAZbmbZfl9n1KRBwOtAQmb+X40IgoiYiS5cuX13qhkiRJkqT665prruGuu+6q2B45ciQ33ngjp5xyCp06daJdu3b86U9/qjj+s5/9jNatW3Pqqacyf/78iv2LFi2iT58+dO7cme7du/Pqq68CMGTIEK688kpOPvlkrrnmmh33xLSFLG+jGlXsS1tpOxAYn1LaWNXBlNIoYBRAcXHx1vqQJEmSJO2CBg4cyOWXX84ll1wCwMMPP8wTTzzBFVdcQdOmTXnvvfc47rjj6Nu3Ly+++CJjx47lb3/7Gxs2bKBTp0507twZgKFDh3L33XfTqlUrXnjhBS655BImTy7/O/trr73GpEmTKCoqqrPnuavLMsAoAw7N224OvLWVtgOBH2RYiyRJkiRpJ9WxY0feffdd3nrrLZYvX86+++5Ls2bNuOKKK5g6dSoNGjTgzTff5J133uG5556jX79+7LHHHgD07dsXgNWrV/P888/Tv3//in4//vjjisf9+/c3vKhjWQYYM4FWEdESeJPykGJw5UYR0RrYF5ieYS2SJEmSpJ3YOeecw/jx43n77bcZOHAgY8aMYfny5cyaNYtGjRrRokUL1q1bB0DEpycMbNq0iX322YfS0tIq+99zzz0zrV/VyyzASCltiIhhwJNAEXBfSmluRNwElKSUJuaaDgLGppScGiKpThSPquVVsmtJydDsVtuWJEna2QwcOJCLLrqI9957j2effZaHH36YAw88kEaNGvHMM8/wxhtvANCjRw+GDBnCiBEj2LBhA4899hjf+973aNq0KS1btuSRRx6hf//+pJR46aWXaN++fR0/M22W5QgMUkqPA49X2ndDpe2RWdYgSfVWbd9+rDZleCszSZK0E6iDnxXatGnDhx9+yCGHHEKzZs0499xz+eY3v0lxcTEdOnTgyCOPBKBTp04MGDCADh06cPjhh9O9e/eKPsaMGcP3v/99fvrTn7J+/XoGDhxogFFAMg0wJEmSJEnaUV5++eWKx/vvvz/Tp1e9UsF1113Hdddd96n9LVu25IknnvjU/tGjR9dajfr8sryNqiRJkiRJUq0wwJAkSZIkSQXPAEOSJEmSJBU8AwxJkiRJklTwXMRTkvSZlRQX5l1IiksK+M4tkiRJ2i6OwJAkSZIkSQXPERiSJEmSpFpV26M1azLKsqysjB/84AfMmzePTZs2cfrpp3Pbbbex2267fe7rnnTSSSxevJg33niDiADgzDPPZNKkSaxevbrG/QwZMoTTTz+dc845Z7va7OocgSFJkiRJqtdSSpx11lmceeaZLFiwgNdee43Vq1dz3XXXbXff++yzD//3f/8HwMqVK1m2bNl296nPxxEYO0hxAU/LLinMqeySJEmSVCOTJ0+mcePG/Ou//isARUVF3H777bRs2ZJZs2bx61//mmOOOYaOHTvSr18/brjhBn784x9z+OGH8+Uvf5mRI0ey//77M2fOHDp37syDDz5YMeJi4MCBjB07lm7dujFhwgTOOuss5s6dC5QHJz/60Y/43//9XyKC66+/ngEDBpBS4tJLL2Xy5Mm0bNmSlFJFrbNmzeLKK69k9erV7L///owePZpmzZrt+BetHnIEhiRJkiSpXps7dy6dO3feYl/Tpk057LDDOOmkk3juuef44IMPaNiwYcVoimnTptG9e3cA/va3v/HrX/+aefPmsXjx4oo2AKeccgpTp05l48aNjB07lgEDBlQcmzBhAqWlpcyePZtJkyYxfPhwli1bxh//+Efmz5/Pyy+/zL333svzzz8PwPr167n00ksZP348s2bN4sILL6yVUSK7CkdgSJIkSZLqtZRSxYiJyvt79uzJXXfdRcuWLfnGN77B008/zdq1a1myZAmtW7dm2bJldO3alebNmwPQoUMHlixZQrdu3YDy0RzdunVj3LhxfPTRR7Ro0aKi/2nTpjFo0CCKioo46KCD6NmzJzNnzmTq1KkV+//lX/6Fr371qwDMnz+fOXPm0KtXLwA2btzo6IvPwABDkiRJklSvtWnThv/+7//eYt8HH3zA0qVL6dixIyUlJRxxxBH06tWL9957j3vvvXeLERu77757xeOioiI2bNiwRV8DBw6kX79+jBw5cov9+VNDKttaoNKmTRumT5/+WZ6ecpxCIkmSJEmq10455RTWrl3LH/7wB6B8ZMNVV13FkCFDaNq0KYceeigPP/wwxx13HN27d+eXv/xlxfSRmujevTvXXnstgwYN2mJ/jx49GDduHBs3bmT58uVMnTqVrl270qNHD8aOHcvGjRtZtmwZzzzzDACtW7dm+fLlFQHG+vXrK9bTUPUcgSFJkiRJqlU1ue1pbYoI/vjHP3LJJZdw8803s2nTJr7+9a/z85//HCgPIP7yl7+wxx570L17d8rKyj5TgBERXH311Z/a369fP6ZPn0779u2JCG699VYOPvhg+vXrx+TJk2nXrh1f+cpX6NmzJwC77bYb48eP57LLLmPVqlVs2LCByy+/nDZt2tTOC7GTi20NeSlExcXFqaQe3jbDu5DsXGr7vta1aUd/s9gZFI8qzNesZFRdV7B1Jdxd1yVUqZDf/34fkCTtzF555RWOOuqoui5D9UBV75WImJVSqvanJaeQSJIkSZKkgmeAIUmSJEmSCp4BhiRJkiRpu9W35Qm0423ve8QAQ5IkSZK0XRo3bsz7779viKGtSinx/vvv07hx48/dh3chkSRJkiRtl+bNm1NWVsby5cvruhQVsMaNG9O8efPPfb4BhiRJkiRpuzRq1IiWLVvWdRnayTmFRJIkSZIkFTwDDEmSJEmSVPAMMCRJkiRJUsEzwJAkSZIkSQUv0wAjIvpExPyIWBgRI7bS5lsRMS8i5kbEQ1nWI0mSJEmS6qfM7kISEUXAnUAvoAyYGRETU0rz8tq0Aq4FTkwp/SMiDsyqHkmSJEmSVH9lOQKjK7AwpbQ4pfQJMBY4o1Kbi4A7U0r/AEgpvZthPZIkSZIkqZ7KMsA4BFiat12W25fvK8BXIuL/IuKvEdEnw3okSZIkSVI9ldkUEiCq2JequH4r4CSgOfBcRLRNKa3coqOIocBQgMMOO6z2K5UkSZIkSQUtywCjDDg0b7s58FYVbf6aUloPvB4R8ykPNGbmN0opjQJGARTQqcccAAAgAElEQVQXF1cOQSRJknaokuKSui6hSsUlxXVdgiRJmclyCslMoFVEtIyI3YCBwMRKbR4FTgaIiP0pn1KyOMOaJEmSJElSPZRZgJFS2gAMA54EXgEeTinNjYibIqJvrtmTwPsRMQ94BhieUno/q5okSZIkSVL9lOUUElJKjwOPV9p3Q97jBFyZ+5AkSZIkSapSllNIJEmSJEmSaoUBhiRJkiRJKngGGJIkSZIkqeAZYEiSJEmSpIJngCFJkiRJkgqeAYYkSZIkSSp4BhiSJEmSJKngGWBIkiRJkqSCZ4AhSZIkSZIKngGGJEmSJEkqeAYYkiRJkiSp4BlgSJIkSZKkgmeAIUmSJEmSCp4BhiRJkiRJKngGGJIkSZIkqeAZYEiSJEmSpIJngCFJkiRJkgpew7ouQJIkSVL9UVJcUtclbFVxSXFdlyApQ47AkCRJkiRJBc8AQ5IkSZIkFTwDDEmSJEmSVPBcA0OSJEmZKy7QpQlKCnc5B0lSJY7AkCRJkiRJBc8AQ5IkSZIkFTwDDEmSJEmSVPAMMCRJkiRJUsEzwJAkSZIkSQUv0wAjIvpExPyIWBgRI6o4PiQilkdEae7ju1nWI0mSJEmS6qfMbqMaEUXAnUAvoAyYGRETU0rzKjUdl1IallUdkiRJkiSp/styBEZXYGFKaXFK6RNgLHBGhteTJEmSJEk7qSwDjEOApXnbZbl9lZ0dES9FxPiIOLSqjiJiaESURETJ8uXLs6hVkiRJkiQVsCwDjKhiX6q0/RjQIqV0DDAJuL+qjlJKo1JKxSml4gMOOKCWy5QkSZIkSYUuywCjDMgfUdEceCu/QUrp/ZTSx7nNe4HOGdYjSZIkSZLqqcwW8QRmAq0ioiXwJjAQGJzfICKapZSW5Tb7Aq9kWI8kSZIkSdulpLikrkvYquKS4rouIVOZBRgppQ0RMQx4EigC7kspzY2Im4CSlNJE4LKI6AtsAFYAQ7KqR5IkSZIk1V9ZjsAgpfQ48HilfTfkPb4WuDbLGiRJkiRJUv2X5RoYkiRJkiRJtcIAQ5IkSZIkFTwDDEmSJEmSVPAMMCRJkiRJUsEzwJAkSZIkSQWv2gAjIoZFxL47ohhJkiRJkqSq1GQExsHAzIh4OCL6RERkXZQkSZIkSVK+agOMlNL1QCvg98AQYEFE/DwivpRxbZIkSZIkSUAN18BIKSXg7dzHBmBfYHxE3JphbZIkSZIkSQA0rK5BRFwGXAC8B/wOGJ5SWh8RDYAFwI+yLVGSJEmSJO3qqg0wgP2Bs1JKb+TvTCltiojTsylLkiRJkiTpn2oyheRxYMXmjYjYKyKOBUgpvZJVYZIkSZIkSZvVJMD4LbA6b3tNbp8kSZIkSdIOUZMAI3KLeALlU0eo2dQTSZIkSZKkWlGTAGNxRFwWEY1yHz8EFmddmCRJkiRJ0mY1CTAuBk4A3gTKgGOBoVkWJUmSJEmSlK/aqSAppXeBgTugFkmSJEmSpCpVG2BERGPgO0AboPHm/SmlCzOsS5IkSZIkqUJNppA8ABwM9AaeBZoDH2ZZlCRJkiRJUr6aBBhfTin9GFiTUrof+AbQLtuyJEmSJEmS/qkmAcb63L8rI6ItsDfQIrOKJEmSJEmSKql2DQxgVETsC1wPTASaAD/OtCpJkiRJkqQ82wwwIqIB8EFK6R/AVOCIHVKVJEmSJElSnm1OIUkpbQKG7aBaJEmSJEmSqlSTNTCejoirI+LQiPji5o/MK5MkSZIkScqpyRoYF+b+/UHevoTTSSRJkiRJ0g5S7QiMlFLLKj5qFF5ERJ+ImB8RCyNixDbanRMRKSKKP0vxkiRJkiRp11DtCIyI+HZV+1NKf6jmvCLgTqAXUAbMjIiJKaV5ldrtBVwGvFDToiVJkiRJ0q6lJlNIuuQ9bgycArwIbDPAALoCC1NKiwEiYixwBjCvUrubgVuBq2tSsCRJkiRJ2vVUG2CklC7N346IvYEHatD3IcDSvO0y4NhKfXUEDk0p/TkithpgRMRQYCjAYYcdVoNLS5IkSZKknUlN7kJS2VqgVQ3aRRX7UsXBiAbA7cBV1XWUUhqVUipOKRUfcMABNS5UkiRJkiTtHGqyBsZj/DN4aAAcDTxcg77LgEPztpsDb+Vt7wW0BaZEBMDBwMSI6JtSKqlB/5IkSZIkaRdRkzUwfpn3eAPwRkqprAbnzQRaRURL4E1gIDB488GU0ipg/83bETEFuNrwQpIkSZIkVVaTAOPvwLKU0jqAiPhCRLRIKS3Z1kkppQ0RMQx4EigC7kspzY2Im4CSlNLE7axdkiRJkiTtImoSYDwCnJC3vTG3r0vVzf8ppfQ48HilfTdspe1JNahFkiRJkiTtgmqyiGfDlNInmzdyj3fLriRJkiRJkqQt1STAWB4RfTdvRMQZwHvZlSRJkiRJkrSlmkwhuRgYExG/yW2XAd/OriRJkiRJkqQtVRtgpJQWAcdFRBMgUkofZl+WJEmSJEnSP1UbYETEz4FbU0orc9v7AlellK7PujhJkiRJUnaKRxXXdQlbVTK0pK5LUIGpyRoYp20OLwBSSv8Avp5dSZIkSZIkSVuqSYBRFBG7b96IiC8Au2+jvSRJkiRJUq2qySKeDwJ/iYj/ym3/K3B/diVJkiRJkiRtqSaLeN4aES8BpwIBPAEcnnVhkiRJkiRJm9VkCgnA28Am4GzgFOCVzCqSJEmSJEmqZKsjMCLiK8BAYBDwPjCO8tuonryDapMkSZIkSQK2PYXkVeA54JsppYUAEXHFDqlKkiRJkiQpz7YCjLMpH4HxTEQ8AYylfA0MSZIkSdJnUFxc1xVsxdC6LkCqua2ugZFS+mNKaQBwJDAFuAI4KCJ+GxFf20H1SZIkSZIkVb+IZ0ppTUppTErpdKA5UAqMyLwySZIkSZKknJrehQSAlNKKlNI9KaWvZlWQJEmSJElSZZ8pwJAkSZIkSaoL21rEU5IkSZKkulGwK5/eXdcF7LIcgSFJkiRJkgqeAYYkSZIkSSp4BhiSJEmSJKngGWBIkiRJkqSCZ4AhSZIkSZIKngGGJEmSJEkqeAYYkiRJkiSp4BlgSJIkSZKkgpdpgBERfSJifkQsjIgRVRy/OCJejojSiJgWEUdnWY8kSZIkSaqfMgswIqIIuBM4DTgaGFRFQPFQSqldSqkDcCvw71nVI0mSJEmS6q8sR2B0BRamlBanlD4BxgJn5DdIKX2Qt7knkDKsR5IkSZIk1VMNM+z7EGBp3nYZcGzlRhHxA+BKYDfgqxnWI0mSJEmS6qksR2BEFfs+NcIipXRnSulLwDXA9VV2FDE0IkoiomT58uW1XKYkSZIkSSp0WQYYZcChedvNgbe20X4scGZVB1JKo1JKxSml4gMOOKAWS5QkSZIkSfVBlgHGTKBVRLSMiN2AgcDE/AYR0Spv8xvAggzrkSRJkiRJ9VRma2CklDZExDDgSaAIuC+lNDcibgJKUkoTgWERcSqwHvgHcEFW9UiSJEmSpPory0U8SSk9Djxead8NeY9/mOX1JUmSJEnSziHLKSSSJEmSJEm1wgBDkiRJkiQVPAMMSZIkSZJU8AwwJEmSJElSwTPAkCRJkiRJBc8AQ5IkSZIkFTwDDEmSJEmSVPAMMCRJkiRJUsEzwJAkSZIkSQXPAEOSJEmSJBU8AwxJkiRJklTwDDAkSZIkSVLBM8CQJEmSJEkFzwBDkiRJkiQVPAMMSZIkSZJU8AwwJEmSJElSwTPAkCRJkiRJBc8AQ5IkSZIkFTwDDEmSJEmSVPAMMCRJkiRJUsEzwJAkSZIkSQXPAEOSJEmSJBU8AwxJkiRJklTwDDAkSZIkSVLBM8CQJEmSJEkFzwBDkiRJkiQVPAMMSZIkSZJU8DINMCKiT0TMj4iFETGiiuNXRsS8iHgpIv4SEYdnWY8kSZIkSaqfMgswIqIIuBM4DTgaGBQRR1dq9jegOKV0DDAeuDWreiRJkiRJUv2V5QiMrsDClNLilNInwFjgjPwGKaVnUkprc5t/BZpnWI8kSZIkSaqnsgwwDgGW5m2X5fZtzXeA/63qQEQMjYiSiChZvnx5LZYoSZIkSZLqg4YZ9h1V7EtVNow4DygGelZ1PKU0ChgFUFxcXGUfkgpbcXFdV7ANQ+u6AEmSJEnVyTLAKAMOzdtuDrxVuVFEnApcB/RMKX2cYT2SJEmSJKmeynIKyUygVUS0jIjdgIHAxPwGEdERuAfom1J6N8NaJEmSJElSPZZZgJFS2gAMA54EXgEeTinNjYibIqJvrtltQBPgkYgojYiJW+lOkiRJkiTtwrKcQkJK6XHg8Ur7bsh7fGqW15ckSZIkSTuHLKeQSJIkSZIk1QoDDEmSJEmSVPAMMCRJkiRJUsEzwJAkSZIkSQXPAEOSJEmSJBU8AwxJkiRJklTwDDAkSZIkSVLBM8CQJEmSJEkFzwBDkiRJkiQVvIZ1XYAkSZJUV4pHFdd1CVtVMrSkrkuQpILiCAxJkiRJklTwDDAkSZIkSVLBM8CQJEmSJEkFzwBDkiRJkiQVPAMMSZIkSZJU8AwwJEmSJElSwTPAkCRJkiRJBc8AQ5IkSZIkFTwDDEmSJEmSVPAMMCRJkiRJUsEzwJAkSZIkSQXPAEOSJEmSJBU8AwxJkiRJklTwDDAkSZIkSVLBM8CQJEmSJEkFzwBDkiRJkiQVvEwDjIjoExHzI2JhRIyo4niPiHgxIjZExDlZ1iJJkiRJkuqvzAKMiCgC7gROA44GBkXE0ZWa/R0YAjyUVR2SJEmSJKn+a5hh312BhSmlxQARMRY4A5i3uUFKaUnu2KYM65AkSZIkSfVclgHGIcDSvO0y4NjP01FEDAWGAhx22GHbX5kkSZJU6IqL67qCrbi7rguQtIvKcg2MqGJf+jwdpZRGpZSKU0rFBxxwwHaWJUmSJEmS6pssA4wy4NC87ebAWxleT5IkSZIk7aSyDDBmAq0iomVE7AYMBCZmeD1JkiRJkrSTyizASCltAIYBTwKvAA+nlOZGxE0R0RcgIrpERBnQH7gnIuZmVY8kSZIkSaq/slzEk5TS48DjlfbdkPd4JuVTSyRJkiRJkrYqyykkkiRJkiRJtcIAQ5IkSZIkFbxMp5CofigeVZj3GC8ZWlLXJUiSJEmSCoQjMCRJkiRJUsEzwJAkSZIkSQXPAEOSJEmSJBU8AwxJkiRJklTwDDAkSZIkSVLBM8CQJEmSJEkFzwBDkiRJkiQVPAMMSZIkSZJU8AwwJEmSJElSwTPAkCRJkiRJBc8AQ5IkSZIkFTwDDEmSJEmSVPAMMCRJkiRJUsEzwJAkSZIkSQXPAEOSJEmSJBU8AwxJkiRJklTwDDAkSZIkSVLBM8CQJEmSJEkFzwBDkiRJkiQVPAMMSZIkSZJU8AwwJEmSJElSwTPAkCRJkiRJBc8AQ5IkSZIkFbyGdV2AJEmqW8Wjiuu6hCqVDC2p6xIkSVIByXQERkT0iYj5EbEwIkZUcXz3iBiXO/5CRLTIsh5JkiRJklQ/ZTYCIyKKgDuBXkAZMDMiJqaU5uU1+w7wj5TSlyNiIPALYEBWNameKS7MvwiWu7uuC5AkSZKkXUqWIzC6AgtTSotTSp8AY4EzKrU5A7g/93g8cEpERIY1SZIkSZKkeihSStl0HHEO0Cel9N3c9vnAsSmlYXlt5uTalOW2F+XavFepr6HA0Nxma2B+JkWrNuwPvFdtK2nn5deAdnV+DWhX59eAdmW+//V5HZ5SOqC6Rlku4lnVSIrKaUlN2pBSGgWMqo2ilK2IKEkpFfLcDylTfg1oV+fXgHZ1fg1oV+b7X1nLcgpJGXBo3nZz4K2ttYmIhsDewIoMa5IkSZIkSfVQlgHGTKBVRLSMiN2AgcDESm0mAhfkHp8DTE5ZzWmRJEmSJEn1VmZTSFJKGyJiGPAkUATcl1KaGxE3ASUppYnA74EHImIh5SMvBmZVj3YYp/poV+fXgHZ1fg1oV+fXgHZlvv+VqcwW8ZQkSZIkSaotWU4hkSRJkiRJqhUGGJIkSZIkqeAZYNQzEdEvIlJEHFnXtWxLREyJiPkRMTsiZkZEh+3o6/GI2Kc261P9EBEbI6I076NFRBRHxB211P/m9+nm/s+phT5HRsTVW9n/Zu468yJi0HZc46aIOHX7KpVqJiJW5/5tkfv+c3Pesf0jYn1E/Ca3nf8+XxAREyLi6Lz2+V9zr0TE0B3/jLSziojrImJuRLyUe48d+zn66BARX8/b7hsRI2q30k9d86SIOCFvu0dEvBgRG6r6vhQRTXNfZ7/J29c5Il6OiIURcUdERJY1q7BExH55P8u8nff/cGnuZgrbOvfJiNgrIr4YERfn7T8iImplfcKI6JL7/nFK3r6iiHhuK+0bRsTK3ONDI2JcbdShnYMBRv0zCJhG/Vjw9NyUUnvgLuC2z9tJSunrKaWVtVeW6pGPUkod8j6WpJRKUkqX1eI1zs3rf3wt9luV21NKHYAzgHsiotHn6SSldENKaVLtlibVyGLg9Lzt/sDcSm1uz309tQLGAZMj4oC84+fmvg5OBH5R3Q/XUk1ExPGUvzc7pZSOAU4Fln6OrjoAFQFGSmliSumW2qlyq04CTsjb/jswBHhoK+1vBp6ttO+3wFCgVe6jT61WqIKWUnp/888ywN388//hDimlT6o5t3dK6UPgi8DFeYeO4DP+vhERW7tBxObfXyr+eJNS2phS6l5dHymlpSmlAZ+lDu3cDDDqkYhoQvkPfN8h7z+UXHL/bEQ8HBGvRcQtEXFuRMzIpfFfyrX7ZkS8EBF/i4hJEXFQbv/jeSntqoi4ICIaR8R/5c7/W0ScnGs7JPcXtSdyf127tQalTwcOyav3axExPffXhUcioklEnBYRD1d6To/lHi+JiP1zj8/LPa/SiLgnl95+KyL+PXf8hxGxOPf4SxExLff4lij/q/dLEfHL7fg0qI7l3ht/zj0eGRH35f6quzgiLstr96n3yme4xpURMSf3cXkN9l8X5X9VngS0rq7/lNICYC2wb+78L+W+pmZFxHMRcWRE7J177zfItdkjIpZGRKOIGB25v8rl/ur2bO7cJyOiWUQcGBGzcsfb5/7qcVhue1Gur/655zE7IqbW9LXRLu8j4JWIKM5tDwAe3lrjlNI44ClgcBWHmwBrgI21XaR2Sc2A91JKHwOklN5LKb1V1f+RUDEa6Be57xOvRUT3XJh2EzAg971jQO7nns0jjEZHxG8j4pnc95yeue9Br0TE6M2FVPVzTm7/koi4Mbf/5dz/9S0o/6Xx/7d350F3T3ccx98fsSWi0RptLZGMEntFLLXFXmupJVomthadNpYKsbRjMFqVGUVbmpaqsYyRZxBKEFKNJggVkcRSQYtaUrGVxJIi3/7xPddz3d773Dw8kfskn9eMyb2/e37nnvv4/X73/L7ne84dXt5zcAnWzwDm135ISZsBXyHPq8q2VYEvRMTkyNX5rwb26/o/sXU3kn4qaVh5fLGku8rj3SvHrKQXlZnOI4F1y3E4sjzfqTw/QZkVcWE5Z2ZIOrrsv6vyvmI08EidNiwFHAgcAexZzrPaLIuGdUhaW9K08vhoSTeUc/lpSedVlduz6rxrk7RCV/4trXU4gNG97AeMi4ingDckDap6bRPgx8DGwGHAgIjYErgcOL6UuRfYKiI2BUYDp8LHGQ4DycDI88DNwLHltY3JaOlVkpYv9QwkO60bk1/yfZu0e49SJ8pAxBnArhExCJgCnASMB7aquth8lxy5+5ik9cv2bUt7PwKGAhOBSgR3MPC6pNWB7YBJkr4E7A9sWEZlft6kvdY6eqo9uHZTgzLrAbsDWwJnlRv8RsdKPddWvcfKpXP4PeAbwFbAMZI2bbL9YGBT4ABgi2Yfqpy7T0fE7LLpMuD4iNgMGAGMioi3gOnADqXMPsCdEfFBVT3LABcDQ8q+VwDnlnqXl/QF8pyYAgyW1A+YHRHvAmcCu5csqX2btdmsymjgYElrkOfWy03KTyXP04prJc0AZgI/iwgHMKwr3AX0LcGIUSW4UPcaWbXP0qWvdCJwVhmpPhNoKyPX9dLWvwjsDAwHbgUuAjYENlZOP2nUz6l4rWz/HTAiIp7jkyPmdVPq4eMbwQuAU2peWh14ser5i1QNHNkSrbqPPAhYSZnhsB1Qe6ydDswsx+Hp5fmE8vw3ZIbP7HLObAEcWxkcIftFp5b7hlrbA09GxD+B+2icHdRRHdU2AYYAXwcOlbSapC+X9u5Szq8Z5H2RLYYapflYazoE+FV5PLo8n1qePxQRsyBHWGmPzD8K7FQerwG0lUj9ssCzlYrLF+41wHci4i1J25Ff+kTEk5KeBwaU4neXmyskPQH0o36a5rUlINGDvGhCXpw2AO5TTs9cFpgcER9KGgfsI+kGYG9KgKXKLsBmwENl357khfTfyiyOFYG+ZMrl9uQFewzwNvA+cLmk24Cx9f641pLeKwGIjtxWRtzmSZpNjkzVPVYa7D80IqZUnkg6FLgpIt4pz8eQx5IabF+qbH+3bL+lg7YOl3QMmZa5Rynfm0wdvl7tU5aXK/+2kYGYCWSQZFRNfesCGwHjy749gFnltfvJjK3tgV+U9xPtHZb7gCuVmU9jOmizWa1xZAr7K9QEmhuonYs/NCKmKKeV3C9pXEQ839WNtCVLRMwtAeXBZL+njRywaHSNhPZr38NA/wV8q1sjIiQ9CrwSEY8CSHq81LEGdfo5Dd7zgAX/hAAMA26PiBf0ySUu6q13EZ2s2xZPDwFblAyLucAz5IDLYLLf3xm7AeurfV2MPuR0Jci+/L8a7HcIed8C7fcv9fpKHdVR7c9lyguSngTWBL5Knnf3V5139y5AXdYNOYDRTUhamYz4byQpyC/hkFS5yZ9XVXx+1fP5tP9/vhi4MCJukbQjcHapuwd5QTknIh6rvGUHzal+r49ofBwNJUeQRwK/Jb+oBYyPiHoLGLaRmR9vkAGZOTWvC7gqIn5SZ9/J5Oj4TPIG7fvA1sDJJTiyJXlTezBwHPm3tMVDveOxo2OlmUbHfkfnxIJ2FC+KiF9KOgC4Wjm9ayngPw0CNbcA55Usos2Av9Rp0+MRsXWdfSeRHZR+wJ+A00o7xwJExA+VC9ztDUyTNDAiXl/Az2FLsIj4r3KK0snkyPM+TXbZlByFrq3nVUlTyawmBzDsMyvZPPcA95QAw7E0vkZC+/dHR32ZRvtU97Uqz5cudTXq53za96zYmsymG0ZOwVpWucjur8nAScUaNM+MsiVARMyT9DJwODlw8RTZH16zZHR3hoBhEXH3JzbmouLv1N0hs6D2B/aSdBbZ51mpDHDOqylet446GvX7xkXEYQtYh3VjnkLSfQwBro6IfhHRPyL6khkU23Wijj7AS+XxEVXbRwIzImJ01baJlJR7SQPI6ObMzja6pLufQU4PWR94ANhW0tql7l6lfshOxyDgGOqP6t0NDClpYihXS+5X1d4R5d9HyNGXeSWbpDfQJyJuJ9NEP/Uvoli30dGx0sxEYL9ybK5AfvFOarJ9f0k9SxZQs5s5ImIMeUN3RES8DTwr6aDSVknapJSbC/yN7JyOrZNqPxNYRbl4HWX6zIZVn+NQcqrKfDIwuBfZgUHS1yLiwYg4E3iNzF4yW1AXAKc1C3pJOpActbuuzmu9yODGPxZKC22JImldSetUbRoI/J3G18hG5gArfoamdNTP+UzvGRFDI2LNiOhP9nmujojTSwbuHElbKYefDycD12bwyT7yJDKw93CdcrXHYe3zO4FhZQpK5Zzr2eS9dyMHJfuW+5c1yalXXT119X5gB0lrlbatUHM9sMWIAxjdxyFA7RoAN1J/YbRGzibT1CeRNywVI4Dd1L4OwL5kqnqPMoLRBhxZWRirsyLiPbKzOyIiXiVX1r6uzIF+gDI3utycjQX2pM40j4h4ggyG3FX2HU8u2gV5Qe4LTCz1vEB76tiKwNiyz1/Jeau2GGtyrDTbdypwJRk4eBC4PCIeabK9DZhGnpMN5y/XOAc4qcxpHgocJWk6+YsO364q10YGIv4vqFfmaw8hf8lhemnDNuW150qxygKd95KZHm+W5+crF5F7rJSZvoDtNiMiHo+Iqxq8XFmM8Gny2N25XPsrrlUuyPYwcGVE1OtIm3VWb3K9rifKdX8Dcj2LutfIDkwANijHcKd/+aCjfk4HbiUD4dOUi4luIelF8ld+Li3TU5r5Ebnu2TNkUPCOzrbdFluTyOm1D0bES8AH1OmrRMQrwJTSNxhJDgj2UC72fQJwKfA0mbX5GLmOS7Msoq64f2mqtP0ocqr8dDKg0SxwaN2UIjxFzszMzMzMzMxamzMwzMzMzMzMzKzlOYBhZmZmZmZmZi3PAQwzMzMzMzMza3kOYJiZmZmZmZlZy3MAw8zMzMzMzMxangMYZmZm1qUkfVR+EvLx8hN8lZ8M7mif/pK69Kf1Sr0nSurV1fWamZnZ588BDDMzM+tq70XEwIjYEPgmsBdwVpN9+gNdHsAATgQcwDAzM1sMOIBhZmZmC01EzAZ+AByn1F/SJElTy3/blKIjgcElc2N4o3KSVpU0sZR7TNLgsn03SZNL2esl9ZZ0ArAaMEHShEXx+c3MzKzrKCIWdRvMzMxsMSJpbkT0rtn2JrAeMAeYHxHvS1oHuC4iNpe0IzAiIr5VyvdqUO5kYPmIOFdSDzK7YjlgDLBnRLwj6TRguYg4R9JzwOYR8drn8+nNzMxsYVl6UTfAzMzMlggq/y4DXCJpIPARMKBB+UblHgKukLQMcHNETJO0A7ABcJ8kgGWByQvnY5iZmdmi4gCGmZmZLVSS1iKDELPJtTBeATYhp7K+32C34fXKRcRESdsDewPXSDofeBMYHxGHLMzPYWZmZouW18AwMzOzhUbSKsDvgUsi5632AWZFxHzgMKBHKToHWLFq17rlJPUDZkfEH4A/AoOAB4BtJa1dyvSSNKBBvWZmZtZNOQPDzMzMupMb1ioAAACuSURBVFpPSdPIaSAfAtcAF5bXRgE3SjoImAC8U7bPAD6UNB24soNyOwKnSPoAmAscHhGvSjoSuE7ScqXcGcBTwGXAHZJmRcROC+nzmpmZ2efAi3iamZmZmZmZWcvzFBIzMzMzMzMza3kOYJiZmZmZmZlZy3MAw8zMzMzMzMxangMYZmZmZmZmZtbyHMAwMzMzMzMzs5bnAIaZmZmZmZmZtTwHMMzMzMzMzMys5f0PR4o49dD+jlQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"baseline_results.csv\")\n",
    "classifiers = list(df[\"classifier\"])\n",
    "df.drop(['overall'], axis=1, inplace=True)\n",
    "headers = list(df)[:-1]\n",
    "data = df.loc[:, df.columns != 'classifier']\n",
    "\n",
    "colours = ['b', 'g', 'r', 'm']\n",
    " \n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "index = np.arange(len(headers))\n",
    "bar_width = 0.2\n",
    "opacity = 0.8\n",
    " \n",
    "for idx, (classifier, (_, results))  in enumerate(zip(classifiers, data.iterrows())):\n",
    "    plt.bar(\n",
    "        index + bar_width * idx,\n",
    "        results,\n",
    "        bar_width,\n",
    "        alpha=opacity,\n",
    "        color=colours[idx],\n",
    "        label=classifier\n",
    "    )\n",
    " \n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Baseline Accuracy')\n",
    "plt.xticks(index + bar_width, headers)\n",
    "plt.legend()\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above its clear the classifiers with the most competitive results are textblob-naivebayes, achieving an overall accuracy of 50% on all test datasets and my own model which had an overall accuracy of 60%. These are great starting points for me to validate the performance of my future models. \n",
    "\n",
    "As for the textblob-pattern and vader classifiers, I am really surprised at their results on some of the datasets. As I stated above a purely _random_ baseline would be expected to achieve results of 33% on each dataset. However, on 3/5 of these datasets they had an accuracy below a random baseline. I dont know why this is the case. If I was to take a guess as to why this happened it would be that the inputs from those datasets are too large for these classifiers to handle and they are designed for smaller sequences of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "Last year, on the 11th of October, Google released [this](https://arxiv.org/abs/1810.04805) paper.\n",
    "\n",
    "> We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "\n",
    "On top of releasing the paper, they also made the weights and source code available for two variations of the model they trained, BERT-base and BERT-large. The difference between these two models is the size.  BERT-base has a total of 110M trainable parameters and BERT-large has a whopping 340M (at the time of the papers release this was the largest recorded NLP deep net). The weights made available are only for the [encoder](https://www.quora.com/What-is-an-Encoder-Decoder-in-Deep-Learning) layer of the BERT architecture, as these models were trained to inevitably have their decoder layer removed and then have a new head attached to [fine tune](https://medium.com/fintechexplained/how-to-fine-tune-your-machine-learning-models-to-improve-forecasting-accuracy-e18e67e58898) them on a downstream task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*This is a [PyTorch](https://pytorch.org/) layer summary of BERT-base's encoder with an output embedding size of 768*\n",
    "\n",
    "    BERTEncoder(\n",
    "      (bert): BertModel(\n",
    "        (embeddings): BertEmbeddings(\n",
    "          (word_embeddings): Embedding(28996, 768)\n",
    "          (position_embeddings): Embedding(512, 768)\n",
    "          (token_type_embeddings): Embedding(2, 768)\n",
    "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "        (encoder): BertEncoder(\n",
    "          (layer): ModuleList(\n",
    "            (12): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1)\n",
    "              )\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "        (pooler): BertPooler(\n",
    "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "          (activation): Tanh()\n",
    "        )\n",
    "      )\n",
    "   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-as-service\n",
    "\n",
    "I initially planned on instantiating a pre-trained BERT model by studying the [source code](https://github.com/google-research/bert) and implementing the code. However, David put me onto [bert-as-service](https://github.com/hanxiao/bert-as-service), which made it almost effortless to get the model up and running. Below I describe the simple process of using bert-as-service. This process assumes you already have already installed [python](https://www.python.org/) and [tensorflow](https://www.tensorflow.org/) and downloaded the weights for one of the BERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 open a command line and enter the following commands to install bert-as-service using pythons package manager pip**\n",
    "\n",
    "    pip install bert-serving-server\n",
    "    pip install bert-serving-client\n",
    "\n",
    "bert-serving-server is where all the magic happens. Users can connect to this server using the bert-serving-client module and send pieces of text to the server. These pieces of text will be preprocessed and fed through the BERT model and a n-dimensional vector will be returned for each piece of text. This vector is from the final output layer of the BERT model and is refferred to as a sentence embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 start the bert-serving-server.**\n",
    "\n",
    "![](images/bert-as-service1.png)\n",
    "\n",
    "- *bert-serving-server* is the command that needs to be run.\n",
    "\n",
    "- *model_dir* is the path to the bert model on disk.\n",
    "\n",
    "- (optional) *num_workers* is useful to process datasets faster as the processing of each piece of text is a single job and can be done in parrallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/bert-as-service2.png)\n",
    "                        \n",
    "After the running the command above, if all goes well, your prompt should contain the same text in the red box above. This means that the server has successfully setup and is now ready to recieve requests from clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 is to communicate with the server as a client.**\n",
    "\n",
    "\n",
    "1|2\n",
    ":-------------------------:|:-------------------------:\n",
    "![](images/bert-as-service3.png) |  ![](images/bert-as-service4.png)\n",
    "\n",
    "The left image is an example script a client would use to send text to the server. There is no limit on the amount of pieces of text that can be sent to the server at any time (will just take longer to process).\n",
    "\n",
    "The right image is an example a 1024 dimensional embedding, of the sentence \"This is a test!\", returned by the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "I took the custom training and test sets I created, encoded each sample into 768 and 1024 dimensional embeddings using bert-as-service and then saved them to two new training and test set pairs, one for 768 and the other for 1024 embedding sizes. \n",
    "\n",
    "I trained a few randomly chosen models, on the training set, to get a vague idea of the performance of the bert embeddings and was seeing around 60% accuracy on the validation set. It was at this point that David suggested that I compare some of the embeddings generated from bert-as-service using [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity), as a sanity check, to validate the usefulness of the embeddings. I discovered that embeddings of sentences that are not related were still returning high positive values close to 1 from this check. This obviously set off alarms as one would expect numbers closer to or less than 0 for unrelated sentences. I couldn't explain why this was the case until I put in tested sentences with varying lengths. The results of this test were numbers closer to zero, regardless if the sentences were related or not. This is when I discovered that the bert-as-service server has a parameter for sentence length that defaults to 15 words. Therefore, when encoding sentences greater than 15 words, words after the first 15 are discarded and not included in the generation of the sentence embeddings.\n",
    "\n",
    "With the knowledge in hand, I proceeded to then delete the ecnoded datasets I had generated, as they had many samples with lengths greater than 15 words. I then changed the word count parameter to 150 words in the server and re-encoded the datasets the same way as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "With my newly created datasets finished and a few test models, to make sure everything was working properly, I then moved on to creating a grid search to find a high performance model.\n",
    "\n",
    "From [this](https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998) article a grid search is described as:\n",
    "> the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.\n",
    "\n",
    "This technique usually requires alot of time/resources as a large amount of different models are trained and evaluated. Generally this wouldn't be possible to someone like myself to run because of the limited amount resources that are available. However, with the BERT embeddings that I know had, only a small feed foward network needs to be trained on these to achieve good results. Feed forward networks are incredibly parallelizable, which means training times, especially on a GPU, are quick. It is because of this I decided to create 7 dimensional grid search. The 7 dimensional part referring to the number of hyper parameters that I would be searching for the optimal setting.\n",
    "\n",
    "These 7 hyper parameters I chose were:\n",
    "   - Embedding Size - 768 or 1024 - The different sizes of the embedding, from the two different BERT models, return by bert-as-service\n",
    "   \n",
    "   \n",
    "   - validation_splits = [0.1, 0.2] - Validation split is described [here](https://radiopaedia.org/articles/validation-split-machine-learning). I chose to test two different validation set sizes to see if the model would be able to reach similar results with less training data.\n",
    "   \n",
    "   \n",
    "   - batch_sizes = [64, 128, 256] - Mini batchs are described [here](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/). I tested multiple different batch sizes to find whether allowing the the model to converge quickly or slowly was optimal.\n",
    "   \n",
    "   \n",
    "   - optimizers = [\"sgd\", \"rmsprop\", \"adam\"] - Optimizers are described [here](https://blog.algorithmia.com/introduction-to-optimizers/). The optimizers I chose are all commonly used on various machine learning tasks and I wanted to see which one fit my model the best.\n",
    "   \n",
    "   \n",
    "   - dropout_rates = [0.1, 0.2, 0.3, 0.4] - Dropout is described [here](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5). The use of dropout as a regularization tool in machine has become increasingly more popular and is present in most machine learning models these days. Its no surprise then, that I chose to use it in my model. The values I chose are basically just the stand values.\n",
    "   \n",
    "   \n",
    "   - activations = [\"relu\", \"tanh\", \"sigmoid\"] - Activation functions are described [here](https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/). Same as with the optimizers, these are some of the most commonly used activation functions used in machine learning models. The right activation function for each layer can be very impactful on a models results, so this hyper parameter, in my opinion, is one of, if not the, most important to find the optimal value in.\n",
    "    \n",
    "   \n",
    "   - dense_layers = [1,2,3] => [32, 64, 128, 256] - The final two parameters are related so I included then together here. The first parameter here is the **number of dense layers**. A dense layer is just a fully connected layer in the model. Because the original BERT paper stated that only a small head was needed on top of the encoder I chose a maximum of 3 layers that would be part of a model. The second parameter is the **number of units** that a dense layer has. Each dense layer can have a different value for its number of units and I wanted to find the optimal number of dense layers and the optimal number of units for each of those layers.\n",
    "\n",
    "Together, all of these hyper parameters, resulted in *36288* unique model combinations which made up my grid search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The grid search took 6 days to complete. Throughout this time I encountered many bugs, that stopped it running. These bugs included a memory leak, certain models being skipped because of poorly formatted naming strings and just general feedback information being incorrect. Instead of starting the grid search from scratch every time and having to retrain alot of the models that had been trained, I implemented a feature that allowed training to be resumed from the last trained model. At the conclusion of the grid search I had some code that took all the results of the trained models so far and compiled them into a single csv. Because of the starting and stopping I had to create a script that took the results from all the models that had been trained at a time previous to the last run and combine them into the csv file.\n",
    "\n",
    "I then created another scrip that went throught each model and combined the score that it achieved on each test set and returned the name of the model with the best overall score.\n",
    "\n",
    "The naming string of the model was: **E-1024 VS-0.1 BS-128 OPT-adam DO-0.1 ACT-tanh D-256D-256D-128**\n",
    "\n",
    "Alot to unpack from this string:\n",
    "\n",
    "- E-1024: This is the size of bert embedding that performed the best. Not really surprising as the model with 1024 dimensional embeddings has much more weights than the 784 dimensional model and has therefore has the capacity to capture much more from its training.\n",
    "\n",
    "\n",
    "- **VS-0.1**: The validation split size. I believe that this the case because the model had more training data to learn from. The dataset I used only had around 50,000 samples all up, which is rather small. If I had have used a larger training set, say 150,000-200,000, I would expect that this number would have been the other option 0.2.\n",
    "\n",
    "\n",
    "- **BS-128**: The mini batch size. I thought that 64 would be the optimal value here as the models are very small so fast convergence seemed like the idea best fit.\n",
    "\n",
    "\n",
    "- **OPT-adam**: This isn't really that surprising. Adam has proven to do very well on text classification tasks in the past, so it doing very well here just helps validate that for me. In the future I dont think I will keep this as a parameter in my grid searchs, but make it the default optimizer I use.\n",
    "\n",
    "\n",
    "- **D0-0.1**: The dropout rate. Because the size of the models were very small, I think that dropout really wasnt that effective. Regrettably, I didnt add a no dropout parameter. This would be an interesting test in the future to see if dropout was in fact needed at all.\n",
    "\n",
    "\n",
    "- **ACT-tanh**: The activation function. Very surprising as the tanh activation function is mainly used in binary classifcation problems. My guess is that is because it maps its inputs to values between -1 and 1 that it works well with the three labels that are in the range -1 and 1.\n",
    "\n",
    "\n",
    "- **D-256D-256D-128**: The number of dense layers and their respective units. Three layers were optimal. For future tests I could add some more layers and allow more units in each layer to see if my grid search did not allow enough space.\n",
    "\n",
    "\n",
    "The models test results were as follows:\n",
    "\n",
    "- On the **Amazon Reviews** dataset it achieved 86.32% accuracy. This was a 9% increase over that the score that the best baseline model achieved, on this dataset, which was 77.86%.\n",
    " \n",
    " \n",
    "- On the **Fine Food Reviews** dataset it achieved 81.32% accuracy. This was a 12% increase over that the score that the best baseline model achieved, on this dataset, which was 69.05%.\n",
    " \n",
    " \n",
    "- On the **IMDB** dataset it achieved 84.52% accuracy. This was a 6% increase over that the score that the best baseline model achieved, on this dataset, which was 78.46%.\n",
    " \n",
    " \n",
    "- On the **Sentiment140** dataset it achieved 76.26% accuracy. This was a 23% increase over that the score that the best baseline model achieved, on this dataset, which was 53.73%.\n",
    " \n",
    " \n",
    "- On the **Twitter Airline** dataset it achieved 78.10% accuracy. This was a 29% increase over that the score that the best baseline model achieved, on this dataset, which was 49.05%.\n",
    " \n",
    " \n",
    "The model achieved an overall score of 81.30% on all test sets combined. This was a 21% increase over the best baseline on all test sets combined, which was 60.53%.\n",
    "\n",
    "\n",
    "Im not going to lie, when I saw these results I was pretty gobsmacked. I didnt expect to even achieve, let alone exceed, the 80% accuracy that David set for me after my experiences with how difficult it was last semester.\n",
    "\n",
    "All model results can be found within the project repo at \"bert-modeling/bert-as-service/all results.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "\n",
    "In conclusion, im very happy with the way this semester turned out. At first I was rather hesitant at continuing on with my sentiment analysis engine project. I had such lackluster results last semester and also did not really feel as though there was a that much more interesting stuff that I had to learn when it came to sentiment analysis. I really would have regretted that now as I was finally able to overcome the goal that was set for me last semester, which felt like a life time ago. I learnt alot more than I expected to as well. From learning the correct proceedures to follow when combining multiple dataset, making a GUI using Tkinter, learning to use bert-as-service and finally making a much more complex grid search, I feel this semester,though long and at some point very stagnant, was a great success.\n",
    "\n",
    "So whats next for this project? Well after seeing the results I was achieving, David said he wishes to use my model to aid in his own research. So, I had to create an API for him to interact with my model. I also plan to creat a web application around the model, so I can show it off to friends and family. It will also be a very good asset to show off to future employers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Technical and Professional Development\n",
    "\n",
    "On top of my project, I was required to complete at least 3 pieces of technical/professional and voluntary work. At least 1 being technical/professional and 1 being voluntary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development\n",
    "\n",
    "1. *07/05/19 - Code Craft monthly meetup - Otago Polytech, D-BLOCK* - Two part talk on [Git Workflow](https://slides.com/thumper/git-workflow#/) and [History of Distributed Version Control Software](https://slides.com/thumper/dvcs-history#/) by Tim Penhey, software developer of 25 years, currently working at [canonical](https://www.canonical.com/).\n",
    "\n",
    "    - The first part of Tim's talk, which was about the history of version control software, I found really interesting and informative. The only version control software I have ever used, or even heard of, is git. Honestly I have never even questioned if there was alternatives out there, git just seems to be the thing. I learnt all the way from the earliest VCS's that only one person could use and also about the two contenders\n",
    "    \n",
    "    - The second part of the talk was about how Tim uses git in his daily workflow. He introduces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voluntary Work\n",
    "\n",
    "Mosgiel Library Drop-in centre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
